{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/a/Documents/DataScience_World/ML10_end_to_end/dsproject/CompleteDSproject/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/a/Documents/DataScience_World/ML10_end_to_end/dsproject/CompleteDSproject'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from src.datascience import logger\n",
    "from src.datascience.utils.common import read_yaml, create_directories\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_path: Path\n",
    "    transformed_data_path: Path\n",
    "    target_column: str = \"health_risk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datascience.constants import *\n",
    "from src.datascience.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=Path(\"config/config.yaml\"),\n",
    "        params_filepath=Path(\"params.yaml\"),\n",
    "        schema_filepath=Path(\"schema.yaml\")\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(self.config.data_ingestion.local_data_file),\n",
    "            preprocessor_path=Path(config.root_dir) / \"preprocessor.joblib\",\n",
    "            transformed_data_path=Path(config.root_dir) / \"transformed_data.csv\",\n",
    "            target_column=self.schema.TARGET_COLUMN.name\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.label_encoders = {}\n",
    "        self.target_encoder = LabelEncoder()\n",
    "        \n",
    "    def get_data_transformer(self):\n",
    "        \"\"\"\n",
    "        Creates the data transformation pipeline\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Categorical columns (excluding target and ID)\n",
    "            categorical_columns = [\n",
    "                'product_name', 'brand', 'category', 'adulterant',\n",
    "                'detection_method', 'severity', 'action_taken'\n",
    "            ]\n",
    "            \n",
    "            # Date column\n",
    "            date_column = ['detection_date']\n",
    "            \n",
    "            # Create preprocessing steps for categorical and date features\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('label_encoder', self.CustomLabelEncoder())\n",
    "            ])\n",
    "            \n",
    "            date_transformer = Pipeline(steps=[\n",
    "                ('date_converter', self.DateFeatureExtractor())\n",
    "            ])\n",
    "            \n",
    "            # Combine all transformers\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('cat', categorical_transformer, categorical_columns),\n",
    "                    ('date', date_transformer, date_column)\n",
    "                ],\n",
    "                remainder='passthrough'\n",
    "            )\n",
    "            \n",
    "            return preprocessor\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in creating data transformer: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    class CustomLabelEncoder:\n",
    "        \"\"\"Custom transformer for label encoding with handling for unknown values\"\"\"\n",
    "        def __init__(self):\n",
    "            self.encoders = {}\n",
    "            \n",
    "        def fit(self, X, y=None):\n",
    "            X = pd.DataFrame(X)\n",
    "            for column in X.columns:\n",
    "                self.encoders[column] = LabelEncoder()\n",
    "                self.encoders[column].fit(X[column])\n",
    "            return self\n",
    "            \n",
    "        def transform(self, X):\n",
    "            X = pd.DataFrame(X)\n",
    "            X_encoded = X.copy()\n",
    "            for column in X.columns:\n",
    "                encoder = self.encoders[column]\n",
    "                X_encoded[column] = X[column].map(\n",
    "                    lambda x: -1 if x not in encoder.classes_ else encoder.transform([x])[0]\n",
    "                )\n",
    "            return X_encoded\n",
    "            \n",
    "    class DateFeatureExtractor:\n",
    "        \"\"\"Custom transformer for extracting features from dates\"\"\"\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "            \n",
    "        def transform(self, X):\n",
    "            X = pd.DataFrame(X)\n",
    "            date_df = pd.to_datetime(X.iloc[:, 0])\n",
    "            return pd.DataFrame({\n",
    "                'year': date_df.dt.year,\n",
    "                'month': date_df.dt.month,\n",
    "                'day': date_df.dt.day\n",
    "            })\n",
    "    \n",
    "    def transform_data(self):\n",
    "        \"\"\"\n",
    "        Transforms the data using the preprocessing pipeline\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the data\n",
    "            df = pd.read_csv(self.config.data_path)\n",
    "            \n",
    "            # Separate features and target\n",
    "            X = df.drop(columns=[self.config.target_column, 'adulteration_id'])\n",
    "            y = df[self.config.target_column]\n",
    "            \n",
    "            # Create and fit the preprocessor\n",
    "            preprocessor = self.get_data_transformer()\n",
    "            X_transformed = preprocessor.fit_transform(X)\n",
    "            \n",
    "            # Transform target variable\n",
    "            y_transformed = self.target_encoder.fit_transform(y)\n",
    "            \n",
    "            # Create feature names\n",
    "            feature_names = (\n",
    "                [f\"{col}_{i}\" for col, n_cols in zip(['product_name', 'brand', 'category', 'adulterant',\n",
    "                                                     'detection_method', 'severity', 'action_taken'], \n",
    "                                                    [1]*7) for i in range(n_cols)] +\n",
    "                ['year', 'month', 'day']\n",
    "            )\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            transformed_df = pd.DataFrame(\n",
    "                X_transformed,\n",
    "                columns=feature_names\n",
    "            )\n",
    "            transformed_df['target'] = y_transformed\n",
    "            \n",
    "            # Save the preprocessor and transformed data\n",
    "            create_directories([self.config.root_dir])\n",
    "            joblib.dump(preprocessor, self.config.preprocessor_path)\n",
    "            transformed_df.to_csv(self.config.transformed_data_path, index=False)\n",
    "            \n",
    "            # Save target encoder\n",
    "            joblib.dump(self.target_encoder, \n",
    "                       Path(self.config.root_dir) / \"target_encoder.joblib\")\n",
    "            \n",
    "            return transformed_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in transforming data: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-08 12:11:31,687: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-01-08 12:11:31,690: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-01-08 12:11:31,694: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-01-08 12:11:31,695: INFO: common: created directory at: artifacts]\n",
      "[2025-01-08 12:11:31,698: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-08 12:11:31,699: INFO: 314665959: Starting data transformation...]\n",
      "[2025-01-08 12:11:32,577: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-01-08 12:11:32,587: INFO: 314665959: Data transformation completed successfully]\n",
      "[2025-01-08 12:11:32,588: INFO: 314665959: Transformed data shape: (1000, 11)]\n"
     ]
    }
   ],
   "source": [
    "# Main function to run data transformation\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_transformation_config)\n",
    "        \n",
    "    logger.info(\"Starting data transformation...\")\n",
    "    transformed_data = data_transformation.transform_data()\n",
    "    logger.info(\"Data transformation completed successfully\")\n",
    "    logger.info(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data transformation: {str(e)}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "completeds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
