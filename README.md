<div align="center">

# ğŸ§ª Food Adulteration Detection System ğŸ§ª

[![FastAPI](https://img.shields.io/badge/FastAPI-0.110.0-009688?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/Python-3.9-3776AB?style=for-the-badge&logo=python)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=for-the-badge&logo=docker)](https://www.docker.com/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-Latest-F7931E?style=for-the-badge&logo=scikit-learn)](https://scikit-learn.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-Latest-FF6F00?style=for-the-badge&logo=tensorflow)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-GPL--3.0-blue?style=for-the-badge)](LICENSE)

*An end-to-end machine learning system for detecting and analyzing food adulteration risks*

</div>

## ğŸ” Project Overview

The **Food Adulteration Detection System** is a comprehensive data science and machine learning platform designed to identify, analyze, and predict health risks associated with food product adulteration. This system combines state-of-the-art machine learning techniques with a modern web API to provide real-time analysis and prediction capabilities.

The project implements a complete MLOps workflow including data ingestion, validation, transformation, model training, evaluation, and deployment - all wrapped in a user-friendly FastAPI interface for seamless interaction.

## âœ¨ Key Features

- ğŸ¤– **Advanced ML Models**: Utilizes ensemble learning techniques to predict adulteration risks
- ğŸ”„ **End-to-End Pipelines**: Automated workflows from data ingestion to prediction
- ğŸŒ **Modern API**: FastAPI-powered interface with automatic OpenAPI documentation
- ğŸ“Š **Interactive UI**: Clean, responsive web interface for submitting samples and viewing results
- ğŸ³ **Containerization**: Docker support for consistent deployment across environments
- ğŸ§ª **Comprehensive Testing**: Pytest-based test suite with coverage reporting
- ğŸ“ **Detailed Logging**: Structured logging for monitoring and debugging

## ğŸ—ï¸ Project Architecture

```mermaid
graph TD
    A[Data Sources] --> B[Data Ingestion]
    B --> C[Data Validation]
    C --> D[Data Transformation]
    D --> E[Model Training]
    E --> F[Model Evaluation]
    F --> G[Model Deployment]
    G --> H[FastAPI Service]
    H --> I[User Interface]
```

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ ğŸ“ .github/                      # GitHub Actions workflows
â”œâ”€â”€ ğŸ“ .git/                         # Git repository
â”œâ”€â”€ ğŸ“ artifacts/                    # Generated model artifacts
â”‚   â”œâ”€â”€ ğŸ“ data_transformation/      # Preprocessors and encoders
â”‚   â””â”€â”€ ğŸ“ model_trainer/            # Trained models
â”œâ”€â”€ ğŸ“ config/                       # Configuration files
â”‚   â””â”€â”€ ğŸ“„ config.yaml               # Main configuration
â”œâ”€â”€ ğŸ“ research/                     # Research notebooks and experiments
â”‚   â”œâ”€â”€ ğŸ“„ data_exploration.ipynb
â”‚   â”œâ”€â”€ ğŸ“„ model_experimentation.ipynb
â”‚   â””â”€â”€ ğŸ“„ feature_engineering.ipynb
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â””â”€â”€ ğŸ“ datascience/             # Main package
â”‚       â”œâ”€â”€ ğŸ“ components/           # Core ML components
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_ingestion.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_validation.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_transformation.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ model_trainer.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ model_evaluation.py
â”‚       â”œâ”€â”€ ğŸ“ config/               # Configuration handlers
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ configuration.py
â”‚       â”œâ”€â”€ ğŸ“ constants/            # Constants and defaults
â”‚       â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“ entity/               # Data entities
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ config_entity.py
â”‚       â”œâ”€â”€ ğŸ“ pipeline/             # Pipeline orchestration
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_ingestion_pipeline.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_validation_pipeline.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_transformation_pipeline.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ model_trainer_pipeline.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ model_evaluation_pipeline.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ prediction_pipeline.py
â”‚       â”œâ”€â”€ ğŸ“ utils/                # Utility functions
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ common.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ exception.py          # Custom exception handling
â”‚       â””â”€â”€ ğŸ“„ logger.py             # Logging configuration
â”œâ”€â”€ ğŸ“ static/                       # Static assets for web UI
â”‚   â”œâ”€â”€ ğŸ“ css/
â”‚   â”œâ”€â”€ ğŸ“ js/
â”‚   â””â”€â”€ ğŸ“ images/
â”œâ”€â”€ ğŸ“ templates/                    # HTML templates
â”‚   â”œâ”€â”€ ğŸ“„ index.html               # Main UI
â”‚   â”œâ”€â”€ ğŸ“„ results.html             # Results display
â”‚   â””â”€â”€ ğŸ“„ error.html               # Error page
â”œâ”€â”€ ğŸ“ tests/                        # Test suite
â”‚   â”œâ”€â”€ ğŸ“„ test_app.py              # API tests
â”‚   â””â”€â”€ ğŸ“„ test_components.py       # Component tests
â”œâ”€â”€ ğŸ“„ .gitignore                    # Git ignore file
â”œâ”€â”€ ğŸ“„ app.py                        # FastAPI application
â”œâ”€â”€ ğŸ“„ Dockerfile                    # Docker configuration
â”œâ”€â”€ ğŸ“„ LICENSE                       # License information
â”œâ”€â”€ ğŸ“„ main.py                       # Application entry point
â”œâ”€â”€ ğŸ“„ metrics.json                  # Model metrics
â”œâ”€â”€ ğŸ“„ params.yaml                   # Model parameters
â”œâ”€â”€ ğŸ“„ pyproject.toml                # Python project configuration
â”œâ”€â”€ ğŸ“„ pytest.ini                    # Pytest configuration
â”œâ”€â”€ ğŸ“„ README.md                     # Project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt              # Dependencies
â”œâ”€â”€ ğŸ“„ schema.yaml                   # Data schema definition
â”œâ”€â”€ ğŸ“„ setup.py                      # Package setup script
â””â”€â”€ ğŸ“„ template.py                   # Template generator
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or later
- Docker (optional for containerization)
- Git

### Installation

#### Local Development

1. **Clone the repository**
   ```bash
   git clone https://github.com/austinLorenzMccoy/CompleteDSproject.git
   cd CompleteDSproject
   ```

2. **Create and activate a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   uvicorn app:app --host 0.0.0.0 --port 8081 --reload
   ```

5. **Access the application**
   - Web UI: http://localhost:8081
   - API Documentation: http://localhost:8081/docs
   - Health Check: http://localhost:8081/health

#### Using Docker

1. **Build the Docker image**
   ```bash
   docker build -t food-adulteration-detection .
   ```

2. **Run the Docker container**
   ```bash
   docker run -p 8081:8081 food-adulteration-detection
   ```

3. **Access the application**
   - Web UI: http://localhost:8081
   - API Documentation: http://localhost:8081/docs

## ğŸ§ª Testing

Run the test suite with pytest:

```bash
python -m pytest tests/ -v
```

For test coverage:

```bash
python -m pytest tests/ --cov=src --cov-report=term-missing
```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web UI for the application |
| `/predict` | POST | API endpoint for prediction (JSON) |
| `/predict-form` | POST | Form submission endpoint |
| `/health` | GET | Health check endpoint |
| `/docs` | GET | Interactive API documentation |

## ğŸ”„ ML Pipeline Components

### Data Ingestion
Handles the collection, import, and initial storage of raw data from various sources.

### Data Validation
Verifies data quality, schema compliance, and performs drift detection.

### Data Transformation
Preprocesses data through cleaning, feature engineering, scaling, and encoding.

### Model Training
Trains machine learning models using optimized hyperparameters.

### Model Evaluation
Assesses model performance using various metrics and validation techniques.

### Prediction Pipeline
Orchestrates the end-to-end process from data input to prediction output.

## ğŸ“ˆ MLflow Experiment Tracking

This project uses MLflow for experiment tracking and model management. MLflow helps track experiments, compare model versions, and manage the model lifecycle.

### MLflow Configuration

The project is configured to use DAGsHub as the MLflow tracking server:

```python
os.environ["MLFLOW_TRACKING_URI"] = "https://dagshub.com/austinLorenzMccoy/CompleteDSproject.mlflow"
os.environ["MLFLOW_TRACKING_USERNAME"] = "austinLorenzMccoy"
os.environ["MLFLOW_TRACKING_PASSWORD"] = "1d06b3f1dc94bb2bb3ed0960c7d406847b9d362d"
```

### Accessing MLflow Tracking

There are two ways to access MLflow tracking information:

1. **DAGsHub MLflow Dashboard**:
   Access experiments directly through the DAGsHub website at:
   ```
   https://dagshub.com/austinLorenzMccoy/CompleteDSproject.mlflow
   ```
   You'll need to log in with your DAGsHub credentials.

2. **Python Script**:
   Use the provided Python script to view experiments programmatically:
   ```bash
   python view_mlflow_experiments.py
   ```

### Tracked Metrics

The following metrics are tracked for each model:
- Accuracy
- Precision
- Recall
- F1 Score

### Model Parameters

Key model parameters tracked in MLflow:
- layer1_units: Number of units in the first layer
- layer2_units: Number of units in the second layer
- learning_rate: Learning rate for model training

## ğŸ³ Docker Configuration

The project includes a comprehensive Dockerfile that:
- Uses Python 3.9 as the base image
- Sets up a secure non-root user
- Configures health checks
- Optimizes layer caching
- Implements best practices for Python containerization

## ğŸ‘¥ Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the GNU General Public License v3.0. See the [LICENSE](LICENSE) file for details.

## ğŸ“ Contact

- **Name**: Chibueze Augustine Chidera
- **Email**: chibuezeaugustine23@gmail.com
- **GitHub**: [austinLorenzMccoy](https://github.com/austinLorenzMccoy)
- **Name**: Emmaculate Likinyo 
---

<div align="center">

### ğŸŒŸ Star this repository if you find it useful! ğŸŒŸ

</div>

## Key Features and Modules

### 1. Components
- **Data Ingestion** (`data_ingestion.py`): Manages data collection and storage.
- **Data Transformation** (`data_transformation.py`): Converts raw data into a format suitable for analysis and modeling.
- **Model Trainer** (`model_trainer.py`): Handles the training of machine learning models.

### 2. Pipeline
- **Training Pipeline** (`training_pipeline.py`): Orchestrates the training process end-to-end.
- **Prediction Pipeline** (`prediction_pipeline.py`): Handles prediction tasks on new data.

### 3. Utilities
- **Common Utilities** (`common.py`): Includes helper functions used throughout the project.

### 4. Configuration
- **Config Module** (`configuration.py`): Loads and parses configuration settings from `config.yaml`.
- **Params** (`params.yaml`): Stores hyperparameters for model training.
- **Schema** (`schema.yaml`): Defines the data structure and constraints.

### 5. Logging and Exception Handling
- **Logger** (`logger.py`): Centralized logging for debugging and monitoring.
- **Custom Exceptions** (`exception.py`): Handles errors in a standardized manner.

### 6. Entity and Constants
- **Entity** (`config_entity.py`): Defines structured configurations such as data paths, model settings, and more to ensure clean and maintainable code.
- **Constants** (`constants/__init__.py`): Stores immutable values like file paths and default parameters, reducing hardcoding and simplifying updates.

### 7. Docker Support
- **Dockerfile**: Ensures a consistent runtime environment for deployment.

### 8. Research and Development
- **Research Notebook** (`research.ipynb`): Used for exploratory data analysis and prototyping.

### 9. Web Interface
- **HTML Template** (`index.html`): Provides a simple user interface for interacting with the project.

## How to Run the Project

### Prerequisites
- Python 3.8 or later
- Docker (optional for containerization)

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/austinLorenzMccoy/CompleteDSproject.git
   cd CompleteDSproject
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up the project:
   ```bash
   python setup.py install
   ```

### Running the Project
1. To execute the training pipeline:
   ```bash
   python src/datascience/pipeline/training_pipeline.py
   ```

2. To run the prediction pipeline:
   ```bash
   python src/datascience/pipeline/prediction_pipeline.py
   ```

### Using Docker
1. Build the Docker image:
   ```bash
   docker build -t datascience-project .
   ```

2. Run the Docker container:
   ```bash
   docker run -it datascience-project
   ```

## Contribution
We welcome contributions! Feel free to fork this repository, make improvements, and submit a pull request.

## License
This project is licensed under the GNU General Public License v3.0. See the [LICENSE](LICENSE) file for details.

## Contact
For inquiries or feedback, please contact:
- **Name**: Chibueze Augustine Chidera
- **Email**: chibuezeaugustine23@gmail.com
- **GitHub**: [austinLorenzMccoy](https://github.com/austinLorenzMccoy)